# üì¶ FMCG Data Engineering Pipeline ‚Äì AtliKon

## üìå Project Overview
This project implements an **end-to-end data engineering pipeline** for an **FMCG (Fast-Moving Consumer Goods)** business use case.  
The pipeline integrates data from **parent and child companies**, processes both **full and incremental loads**, and produces **analytics-ready tables** for dashboarding.

The solution is designed using **real-world data engineering best practices** such as layered architecture, incremental ingestion, and modular notebook-based processing on **Databricks**.

---

## üéØ Business Problem
AtliKon, an FMCG organization, receives sales, customer, product, and pricing data from multiple operational sources:

- **Parent Company**: Provides dimension and fact data
- **Child Company**: Sends daily incremental order data

Challenges addressed:
- Handling **high-volume daily transactional data**
- Supporting **incremental fact loads**
- Maintaining **clean, analytics-ready dimensions**
- Enabling **business dashboards** for decision-making

---

## üèóÔ∏è Architecture Overview
The pipeline follows a **Bronze ‚Üí Silver ‚Üí Gold** layered design:

- **Bronze (Raw)**  
  - Full load and incremental CSV data from parent & child companies
- **Silver (Cleaned & Standardized)**  
  - Validated dimensions and fact tables
- **Gold (Analytics Layer)**  
  - Denormalized tables for dashboarding and reporting

Architecture and design diagrams are available in the `resources/` folder.

---

## üìÇ Project Structure
```text
project-de-fmcg-atlikon
‚îú‚îÄ‚îÄ 0_data/                  # Raw data (ignored from Git)
‚îú‚îÄ‚îÄ 1_codes/
‚îÇ   ‚îú‚îÄ‚îÄ 1_setup/              # Catalog setup, utilities, date dimension
‚îÇ   ‚îú‚îÄ‚îÄ 2_dimension_data_processing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ customers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ products
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pricing
‚îÇ   ‚îî‚îÄ‚îÄ 3_fact_data_processing/
‚îÇ       ‚îú‚îÄ‚îÄ full_load
‚îÇ       ‚îî‚îÄ‚îÄ incremental_load
‚îú‚îÄ‚îÄ 2_dashboarding/
‚îÇ   ‚îú‚îÄ‚îÄ denormalise_table_query_fmcg.txt
‚îÇ   ‚îî‚îÄ‚îÄ fmcg_dashboard.pdf
‚îú‚îÄ‚îÄ resources/
‚îÇ   ‚îú‚îÄ‚îÄ project_architecture.png
‚îÇ   ‚îî‚îÄ‚îÄ databricks_project.excalidraw
‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îî‚îÄ‚îÄ test_data.py
‚îî‚îÄ‚îÄ .gitignore
```
---

## üóÇÔ∏è Raw Data (Bronze Layer)

The Bronze layer contains **raw, unprocessed CSV data** received from upstream source systems.  
These datasets are **intentionally excluded from version control** and documented here to reflect **enterprise data engineering best practices**.

Raw data is organized by **source company** and **load type (full vs incremental)**.

---

## üìå Data Sources & Load Types

| Source Company | Dataset Type | Load Type |
|---------------|-------------|-----------|
| Parent Company | Dimensions & Facts | Full Load |
| Child Company  | Orders | Incremental Load |

---

## üë§ Customers (Raw)

**Source:** Parent & Child Company  
**Load Type:** Full Load  
**Description:** Master data representing customer details.

| Column Name | Description |
|------------|-------------|
| customer_id | Unique customer identifier |
| customer_name | Customer name |
| region | Sales region |
| channel | Sales channel |

---

## üì¶ Products (Raw)

**Source:** Parent & Child Company  
**Load Type:** Full Load  
**Description:** Product master data used for sales analysis.

| Column Name | Description |
|------------|-------------|
| product_id | Unique product identifier |
| product_name | Product name |
| category | Product category |
| brand | Brand name |

---

## üí∞ Pricing (Raw)

**Source:** Parent Company  
**Load Type:** Full Load  
**Description:** Product pricing reference data.

| Column Name | Description |
|------------|-------------|
| product_id | Product reference |
| gross_price | Listed price |
| effective_dt | Price effective date |

---

## üßæ Orders (Raw)

**Source:**  
- Parent Company ‚Äì Full Load (historical)  
- Child Company ‚Äì Incremental Load (daily)

**File Naming Convention (Incremental):**
```text
orders_YYYY_MM_DD.csv
```
---------------------------
### üìÑ Sample Raw Order Data

| order_id | customer_id | product_id | order_date  | quantity | gross_price |
|---------:|-------------|------------|-------------|---------:|------------:|
| 10001    | C101        | P501       | 2025-09-01  | 2        | 120.50      |
| 10002    | C104        | P203       | 2025-09-01  | 1        | 75.00       |
| 10003    | C110        | P509       | 2025-09-01  | 4        | 210.00      |

> Sample shown for illustration only. Actual datasets contain significantly higher volumes.
---------------------------

**üîÑ Data Processing Flow**
---------------------------

### **1Ô∏è‚É£ Setup Layer**

*   Catalog and schema initialization
    
*   Utility functions
    
*   Date dimension creation
    

### **2Ô∏è‚É£ Dimension Processing**

*   Customer dimension
    
*   Product dimension
    
*   Pricing dimension
    
*   Data standardization and validation
    

### **3Ô∏è‚É£ Fact Processing**

*   **Full Load**: Historical fact ingestion
    
*   **Incremental Load**: Daily order data ingestion
    
*   Handles late-arriving data and updates
    

### **4Ô∏è‚É£ Analytics & Dashboarding**

*   Denormalized fact table creation
    
*   Business KPIs and metrics
    
*   Dashboard-ready datasets
    

**üß™ Data Validation & Testing**
--------------------------------

Basic data quality checks are implemented, including:

*   Null checks
    
*   Schema validation
    
*   Record count validation
    
*   Incremental load consistency
    

Tests are available in the test/ directory.

**üìä Dashboard Output**
-----------------------

The final dashboard provides insights such as:

*   Daily and monthly sales trends
    
*   Product-wise revenue
    
*   Customer-level performance
    
*   Price impact analysis
    

A sample dashboard output is included in 2\_dashboarding/fmcg\_dashboard.pdf.

**üõ†Ô∏è Tech Stack**
------------------

*   **Platform**: Databricks
    
*   **Language**: Python, SQL
    
*   **Storage Format**: CSV (raw), Delta (processed)
    
*   **Processing**: Spark
    
*   **Visualization**: SQL-based dashboards
    
*   **Version Control**: Git & GitHub
    

**‚ö†Ô∏è Notes on Data**
--------------------

*   Raw data files are **intentionally excluded** from version control
    
*   .gitignore is used to prevent committing large datasets
    
*   The repository focuses on **pipeline logic and architecture**
    

**üöÄ Key Learnings**
--------------------

*   Designing scalable data pipelines
    
*   Handling incremental fact loads
    
*   Applying real-world data engineering best practices
    
*   Structuring Databricks projects professionally
    

**üìå Future Enhancements**
--------------------------

*   Automate pipelines using Databricks Workflows
    
*   Add advanced data quality checks (Great Expectations)
    
*   Implement Slowly Changing Dimensions (SCD)
    
*   Real-time ingestion using Kafka
